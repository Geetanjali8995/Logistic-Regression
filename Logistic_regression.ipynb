{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Theory Questions**\n",
        "\n",
        "Q.1.  What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Ans:\n",
        "Logistic Regression and Linear Regression are both supervised machine learning algorithms, but they are used for different types of problems and differ in how they model relationships.\n",
        "\n",
        "| Feature           | Linear Regression              | Logistic Regression                 |\n",
        "| ----------------- | ------------------------------ | ----------------------------------- |\n",
        "| Problem Type      | Regression (continuous output) | Classification (categorical output) |\n",
        "| Output            | Real number                    | Probability (0 to 1)                |\n",
        "| Algorithm Goal    | Minimize squared error         | Maximize likelihood (log loss)      |\n",
        "| Function Used     | Linear                         | Sigmoid/logistic function           |\n",
        "| Decision Boundary | Not used                       | Used (e.g., threshold at 0.5)       |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-D0qGDS8cPYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Ans: The mathematical equation of Logistic Regression is based on the sigmoid (logistic) function, which maps any real-valued number into the range between 0 and 1. Here's the step-by-step formulation:\n",
        "\n",
        "1. Linear Combination (Like in Linear Regression):\n",
        "\n",
        "  We first compute the weighted sum of input features:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "=\n",
        "𝛽\n",
        "𝑇\n",
        "𝑥\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        ": Input features\n",
        "\n",
        "𝛽\n",
        "0\n",
        "​\n",
        " : Bias (intercept) term\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛 : Model coefficients (weights)\n",
        "\n",
        "𝑥: Feature vector\n",
        "\n",
        "𝛽: Coefficient vector\n",
        "\n",
        "2. Apply the Sigmoid Function:\n",
        "To convert\n",
        "𝑧 into a probability between 0 and 1:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1/\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "=\n",
        "1/\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "\n",
        "​\n",
        "\n",
        "This gives the predicted probability that the output\n",
        "𝑦\n",
        " is class 1, given the input\n",
        "𝑥.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "48mev5hlcuQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3.  Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Ans: We use the sigmoid function in logistic regression because it transforms the linear output into a probability value between 0 and 1, which is essential for binary classification tasks.\n",
        "\n",
        " Why the Sigmoid Function?\n",
        "1. Probability Output\n",
        "The sigmoid maps any real-valued number (from\n",
        "−\n",
        "∞ to\n",
        "+\n",
        "∞) into a value in the range\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        "):\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1/\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "\n",
        "This output can be interpreted as the probability that the given input belongs to class 1.\n",
        "\n",
        "2. Smooth, Differentiable Curve\n",
        "The sigmoid function is smooth and differentiable, which is important because logistic regression is trained using gradient descent or other optimization methods that rely on derivatives.\n",
        "\n",
        "3. Monotonic Behavior\n",
        "As the input\n",
        "𝑧 increases, the sigmoid function’s output also increases — which makes intuitive sense when modeling likelihoods: more positive evidence increases probability, more negative evidence decreases it.\n",
        "\n",
        "4. Decision Boundary\n",
        "It provides a natural decision threshold at 0.5:\n",
        "\n",
        "If\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "≥\n",
        "0.5, predict class 1\n",
        "\n",
        "If\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "<\n",
        "0.5, predict class 0\n",
        "\n"
      ],
      "metadata": {
        "id": "r3i3MtUofzmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4.  What is the cost function of Logistic Regression?\n",
        "\n",
        "Ans:The cost function of logistic regression is the Log Loss (also called Binary Cross-Entropy Loss). It measures how well the predicted probabilities match the actual binary class labels.\n",
        "\n",
        "Cost Function Definition\n",
        "For binary classification (labels\n",
        "𝑦\n",
        "∈\n",
        "{\n",
        "0\n",
        ",\n",
        "1\n",
        "}\n",
        "), the cost function for a single training example is:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "[\n",
        "𝑦\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        ")\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "- 𝑦 is the actual label (0 or 1)\n",
        "\n",
        "- 𝑦\n",
        "^\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        " is the predicted probability that\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "\n",
        "- 𝜃 is the parameter (weight) vector\n",
        "\n",
        "| Aspect                   | Description                                        |\n",
        "| ------------------------ | -------------------------------------------------- |\n",
        "| Name                     | Log Loss (Binary Cross-Entropy)                    |\n",
        "| Formula (single example) | $-[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) ]$ |\n",
        "| Formula (entire dataset) | Average of the individual losses                   |\n",
        "| Advantage                | Convex, well-suited for classification             |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yrMnbS_wg2P2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5.  What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Ans: Regularization in Logistic Regression is a technique used to prevent overfitting by penalizing large coefficients in the model. It adds a penalty term to the cost function, encouraging the model to keep its weights small and simple.\n",
        "\n",
        "✅ Why Is Regularization Needed?\n",
        "\n",
        "Without regularization:\n",
        "\n",
        "- The model might memorize the training data, especially when the dataset is small or has many features.\n",
        "\n",
        "- This leads to overfitting, where the model performs well on training data but poorly on new/unseen data.\n",
        "\n",
        "Regularization helps the model to generalize better by simplifying it.\n"
      ],
      "metadata": {
        "id": "yRPxIDzQiMd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6. Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "Ans:  Lasso, Ridge, and Elastic Net are all regularized regression techniques used to prevent overfitting and improve model generalization, especially when dealing with many features or multicollinearity.\n",
        "\n",
        "They differ in how they apply penalties to the regression coefficients.\n",
        "\n",
        "| Method          | Penalty Type          | Can Set Coefficients to Zero? | Best For                                         |\n",
        "| --------------- | --------------------- | ----------------------------- | ------------------------------------------------ |\n",
        "| **Ridge**       | L2 (squared weights)  | ❌ No                          | Keeping all features, handling multicollinearity |\n",
        "| **Lasso**       | L1 (absolute weights) | ✅ Yes                         | Feature selection, sparse models                 |\n",
        "| **Elastic Net** | L1 + L2 combo         | ✅ Yes                         | Combining Lasso and Ridge benefits               |\n"
      ],
      "metadata": {
        "id": "WmbLeLBvizcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7.  When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Ans: We should use Elastic Net instead of Lasso or Ridge when:\n",
        "\n",
        "1. You Have Many Correlated Features:\n",
        "  - Lasso tends to randomly pick one of the correlated features and discard the rest.\n",
        "\n",
        "  - Ridge keeps all correlated features, but can make interpretation harder.\n",
        "\n",
        "  - Elastic Net can select groups of correlated features together, balancing both worlds.\n",
        "\n",
        "2. You Need Both Feature Selection and Shrinkage\n",
        "  - Lasso can do feature selection (some weights = 0), but it's unstable when features are correlated or when there are more features than samples.\n",
        "\n",
        "  - Ridge does not perform feature selection at all.\n",
        "\n",
        "  - Elastic Net performs shrinkage like Ridge and feature selection like Lasso.\n",
        "\n",
        "3. High-Dimensional Data (p > n)\n",
        "  - When the number of features 𝑝 is greater than the number of observations 𝑛:\n",
        "\n",
        "    - Lasso may struggle (selecting too few or unstable features).\n",
        "\n",
        "    - Elastic Net tends to perform better because it stabilizes the selection via the L2 part.\n",
        "\n",
        "4. You Want More Robust Models\n",
        "  - Elastic Net is often more stable than Lasso in practice.\n",
        "\n",
        "  - The α parameter (mixing between L1 and L2) gives more flexibility:\n",
        "\n",
        "    - 𝛼\n",
        "=\n",
        "1: Pure Lasso\n",
        "\n",
        "    - 𝛼\n",
        "=\n",
        "0: Pure Ridge\n",
        "\n",
        "    - 0\n",
        "<\n",
        "𝛼\n",
        "<\n",
        "1: Balanced Elastic Net\n",
        "\n"
      ],
      "metadata": {
        "id": "JmnxIzACjMod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "Ans: The regularization parameter\n",
        "𝜆 in logistic regression controls the strength of the penalty applied to the model's coefficients. It plays a crucial role in balancing underfitting and overfitting.\n",
        "\n",
        "**Impact of Different 𝜆 Values**\n",
        "\n",
        "| $\\lambda$ Value      | Model Behavior                                                            | Risk                    |\n",
        "| -------------------- | ------------------------------------------------------------------------- | ----------------------- |\n",
        "| **Very small** (≈ 0) | Almost no regularization → behaves like unregularized logistic regression | **Overfitting**         |\n",
        "| **Large**            | Strong regularization → shrinks weights toward 0                          | **Underfitting**        |\n",
        "| **Moderate**         | Balanced trade-off between fitting and simplicity                         | **Good generalization** |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HbD3B4Xgkn2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Ans: Logistic Regression, like most statistical models, is based on a set of key assumptions. While it's more flexible than linear regression in some ways, violating these assumptions can still hurt model performance or interpretability.\n",
        "\n",
        "**Key Assumptions of Logistic Regression**\n",
        "1. Binary (or Categorical) Outcome\n",
        "- The dependent variable\n",
        "𝑦 should be binary (e.g., 0 or 1) for binary logistic regression.\n",
        "\n",
        "- For multiclass problems, multinomial or one-vs-rest logistic regression is used.\n",
        "\n",
        "2. Linear Relationship Between Features and Log-Odds\n",
        "- Logistic regression does not require a linear relationship between features and the outcome.\n",
        "\n",
        "- But it assumes a linear relationship between the independent variables and the log-odds of the outcome:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "- You can add polynomial or interaction terms if this assumption is violated.\n",
        "\n",
        "3. Independent Observations\n",
        "- Each observation should be independent of the others.\n",
        "\n",
        "- Violations (e.g., repeated measures or time series) require specialized techniques like mixed models or time-series logistic regression.\n",
        "\n",
        "4. Low or No Multicollinearity\n",
        "- Predictor variables should not be highly correlated with each other.\n",
        "\n",
        "- High multicollinearity can:\n",
        "\n",
        "  - Make the model unstable\n",
        "\n",
        "  - Inflate standard errors\n",
        "\n",
        "  - Hurt interpretability\n",
        "\n",
        "- Use Variance Inflation Factor (VIF) to check and address this.\n",
        "\n",
        "5. Large Sample Size\n",
        "- Logistic regression needs enough data to provide reliable estimates, especially when dealing with:\n",
        "\n",
        "  - Rare events\n",
        "\n",
        "  - Many predictors\n",
        "\n",
        "  - Rule of thumb: at least 10–20 observations per predictor.\n",
        "\n",
        "6. No or Minimal Outliers in Predictors\n",
        "- Logistic regression can be sensitive to extreme values in the independent variables.\n",
        "\n",
        "- Scaling, transformations, or robust methods can help.\n",
        "\n"
      ],
      "metadata": {
        "id": "C8JPEUH7lP1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10.  What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Ans:There are many powerful alternatives to logistic regression for classification tasks, especially when logistic regression's assumptions or performance become limiting.\n",
        "\n",
        "Here’s a breakdown of the most common and effective alternatives:\n",
        "\n",
        "1. Decision Trees\n",
        "\n",
        "- How it works: Splits data based on feature values into a tree structure.\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - No need for linear relationships\n",
        "\n",
        "  - Easy to interpret\n",
        "\n",
        "  - Handles both numerical and categorical data\n",
        "\n",
        "- Cons: Can easily overfit (unless pruned or regularized)\n",
        "\n",
        "2. Random Forest\n",
        "\n",
        "- How it works: Ensemble of decision trees (bagging).\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - Robust to overfitting\n",
        "\n",
        "  - Handles non-linearities and interactions\n",
        "\n",
        "  - Works well out-of-the-box\n",
        "\n",
        "- Cons: Less interpretable than logistic regression\n",
        "\n",
        "3. Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost)\n",
        "\n",
        "- How it works: Builds trees sequentially to correct errors of previous ones.\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - State-of-the-art performance on structured data\n",
        "\n",
        "  - Handles missing data and categorical variables well (CatBoost)\n",
        "\n",
        "- Cons: Can be computationally expensive; more tuning required\n",
        "\n",
        "4. Support Vector Machines (SVM)\n",
        "\n",
        "- How it works: Finds the optimal hyperplane to separate classes; supports linear and non-linear boundaries using kernels.\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - Effective in high-dimensional spaces\n",
        "\n",
        "  - Can model non-linear boundaries (with kernels)\n",
        "\n",
        "- Cons: Harder to interpret; tuning can be tricky\n",
        "\n",
        "5. k-Nearest Neighbors (k-NN)\n",
        "\n",
        "- How it works: Classifies based on the majority label among the k closest training examples.\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - Very intuitive\n",
        "\n",
        "  - No training needed\n",
        "\n",
        "- Cons:\n",
        "\n",
        "  - Slow for large datasets\n",
        "\n",
        "  - Sensitive to irrelevant features and data scaling\n",
        "\n",
        "6. Naive Bayes\n",
        "\n",
        "- How it works: Based on Bayes’ Theorem assuming feature independence.\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - Fast and simple\n",
        "\n",
        "  - Works well with text and categorical data\n",
        "\n",
        "- Cons: Assumes feature independence (rarely true)\n",
        "\n",
        "7. Neural Networks (Deep Learning)\n",
        "\n",
        "- How it works: Layers of neurons that learn complex patterns in data.\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - Extremely powerful for non-linear problems\n",
        "\n",
        "  - Great for image, speech, and unstructured data\n",
        "\n",
        "- Cons:\n",
        "\n",
        "  - Requires large datasets\n",
        "\n",
        "  - Less interpretable\n",
        "\n",
        "  - Computationally intensive"
      ],
      "metadata": {
        "id": "5hvQfstCpVfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.11. What are Classification Evaluation Metrics?\n",
        "\n",
        "Ans: Classification evaluation metrics help you measure how well your model is performing in predicting class labels. Choosing the right metric depends on your task (e.g. binary vs. multiclass, imbalanced data, cost of errors).\n",
        "\n",
        "1. Accuracy\n",
        "  - Definition: Ratio of correct predictions to total predictions.\n",
        "\n",
        "       Accuracy=𝑇𝑃+𝑇𝑁/𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁\n",
        "  - Best for: Balanced datasets with equal importance for all classes.\n",
        "\n",
        "  - Limitation: Misleading for imbalanced datasets.\n",
        "\n",
        "2. Precision\n",
        "  - Definition: Of all predicted positives, how many were actually positive?\n",
        "\n",
        "    Precision=𝑇𝑃/𝑇𝑃+𝐹𝑃\n",
        "\n",
        "  - Use when: False positives are costly (e.g., spam filter).\n",
        "\n",
        "3. Recall (Sensitivity, True Positive Rate)\n",
        "  - Definition: Of all actual positives, how many were correctly predicted?\n",
        "\n",
        "    Recall=𝑇𝑃/𝑇𝑃+𝐹𝑁\n",
        "  - Use when: False negatives are costly (e.g., disease detection).\n",
        "\n",
        "4. F1 Score\n",
        "  - Definition: Harmonic mean of precision and recall.\n",
        "\n",
        "    𝐹1=2⋅Precision⋅Recall/Precision+Recall\n",
        "\n",
        "  - Use when: You need a balance between precision and recall.\n",
        "\n",
        "5. Confusion Matrix\n",
        "  - A 2x2 table (for binary) showing:\n",
        "\n",
        "    - TP: True Positives\n",
        "\n",
        "    - FP: False Positives\n",
        "\n",
        "    - TN: True Negatives\n",
        "\n",
        "    - FN: False Negatives\n",
        "\n",
        "  - Helps you visualize where your model is going wrong.\n",
        "\n",
        "6. ROC Curve and AUC (Area Under Curve)\n",
        "  - ROC Curve: Plots True Positive Rate (Recall) vs. False Positive Rate.\n",
        "\n",
        "  - AUC: Measures the area under the ROC curve (ranges 0.0–1.0)\n",
        "\n",
        "    - 1.0 = perfect classifier\n",
        "\n",
        "    - 0.5 = random guessing\n",
        "\n",
        "7. Log Loss (Logarithmic Loss)\n",
        "  - Definition: Measures how close the predicted probabilities are to the true labels.\n",
        "\n",
        "  - Lower is better.\n",
        "\n",
        "  - Used for probabilistic classifiers like logistic regression.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "weKPAjGxqzvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.12.  How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Ans:Class imbalance can significantly affect Logistic Regression and its performance, especially in classification problems. Here’s how:\n",
        "\n",
        "1.Biased Predictions Toward the Majority Class\n",
        "\n",
        "- What happens: Logistic Regression tries to minimize overall error, so if one class dominates (e.g., 95% class 0, 5% class 1), the model may predict the majority class for most inputs.\n",
        "\n",
        "- Result: High accuracy but poor recall/precision for the minority class.\n",
        "\n",
        "2.Poor Decision Boundary\n",
        "\n",
        "- Logistic Regression finds a linear boundary based on the data distribution. With imbalanced data, the minority class has less influence on the boundary, leading to:\n",
        "\n",
        "  - Skewed or suboptimal decision boundaries\n",
        "\n",
        "  - Misclassification of minority class points\n",
        "\n",
        "3.Misleading Evaluation Metrics\n",
        "\n",
        "- Metrics like accuracy become uninformative.\n",
        "\n",
        "- You need to focus on:\n",
        "\n",
        "  - Precision\n",
        "\n",
        "  - Recall\n",
        "\n",
        "  - F1-score\n",
        "\n",
        "  - ROC-AUC\n",
        "\n",
        "  - Confusion matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZsU8b5-tWS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Ans:Hyperparameter tuning in Logistic Regression is the process of finding the best set of hyperparameters — values that control how the model is trained — to improve performance on a given task.\n",
        "\n",
        "What Are Hyperparameters in Logistic Regression?\n",
        "\n",
        "Unlike model parameters (like weights and biases), hyperparameters are set before training and affect how the model learns. Common hyperparameters in Logistic Regression include:\n",
        "\n",
        "| Hyperparameter  | Description                                                                       |\n",
        "| --------------- | --------------------------------------------------------------------------------- |\n",
        "| `C`             | **Inverse** of regularization strength. Smaller values = stronger regularization. |\n",
        "| `penalty`       | Type of regularization: `'l1'`, `'l2'`, `'elasticnet'`, or `'none'`.              |\n",
        "| `solver`        | Algorithm to use for optimization (e.g., `'liblinear'`, `'saga'`, `'lbfgs'`).     |\n",
        "| `max_iter`      | Maximum number of iterations for convergence.                                     |\n",
        "| `class_weight`  | Adjusts weights for imbalanced classes (e.g., `'balanced'`).                      |\n",
        "| `fit_intercept` | Whether to learn the intercept term (`True` or `False`).                          |\n",
        "\n",
        "\n",
        "Why Tune Hyperparameters?\n",
        "\n",
        "- Prevent underfitting or overfitting\n",
        "\n",
        "- Improve accuracy, precision, recall, or F1-score\n",
        "\n",
        "- Adapt the model to specific data characteristics (e.g., imbalance, noise)\n"
      ],
      "metadata": {
        "id": "rClP9YM34bqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.14.  What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Ans:In Logistic Regression, solvers are optimization algorithms used to minimize the loss function and find the best model parameters. Different solvers are better suited for different types of problems.\n",
        "\n",
        "**Common Solvers in Logistic Regression (via scikit-learn)**\n",
        "\n",
        "| Solver      | Supports L1 | Supports L2 | Supports Elastic Net | Multiclass               | Suitable For                  |\n",
        "| ----------- | ----------- | ----------- | -------------------- | ------------------------ | ----------------------------- |\n",
        "| `liblinear` | ✅ Yes       | ✅ Yes       | ❌ No                 | ❌ One-vs-rest (OvR) only | Small datasets, L1 penalty    |\n",
        "| `lbfgs`     | ❌ No        | ✅ Yes       | ❌ No                 | ✅ Multinomial & OvR      | Large datasets, fast & stable |\n",
        "| `sag`       | ❌ No        | ✅ Yes       | ❌ No                 | ✅ Multinomial & OvR      | Large datasets, sparse input  |\n",
        "| `saga`      | ✅ Yes       | ✅ Yes       | ✅ Yes                | ✅ Multinomial & OvR      | Large-scale, sparse, flexible |\n",
        "| `newton-cg` | ❌ No        | ✅ Yes       | ❌ No                 | ✅ Multinomial & OvR      | Large datasets, L2 only       |\n",
        "\n",
        "\n",
        "**How to Choose the Right Solver**\n",
        "\n",
        "- Use liblinear when:\n",
        "  - You have a small dataset\n",
        "\n",
        "  - You want to use L1 regularization (sparse models)\n",
        "\n",
        "  - You're doing binary classification\n",
        "\n",
        "- Use lbfgs when:\n",
        "\n",
        "  - You want fast and stable optimization\n",
        "\n",
        "  - You’re doing multiclass classification\n",
        "\n",
        "  - You use L2 regularization\n",
        "\n",
        "- Use saga when:\n",
        "\n",
        "  - You have a large dataset\n",
        "\n",
        "  - You're working with sparse data\n",
        "\n",
        "  - You want L1, L2, or ElasticNet penalties\n",
        "\n",
        "  - You need multiclass support\n",
        "\n",
        "- Use sag when:\n",
        "\n",
        "  - You have very large datasets\n",
        "\n",
        "  - Data is sparse\n",
        "\n",
        "  - You only use L2 regularization\n",
        "\n",
        "- Use newton-cg when:\n",
        "  - Dataset is large\n",
        "\n",
        "  - You want L2 regularization\n",
        "\n",
        "  - You need multiclass support\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LiqBvJOa4-rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.15.  How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Ans: Logistic Regression is naturally designed for binary classification, but it can be extended to handle multiclass classification in two main ways:\n",
        "\n",
        "1.One-vs-Rest (OvR) (also called One-vs-All)\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "- Trains one binary classifier per class.\n",
        "\n",
        "- Each model predicts whether the input belongs to that class vs. all others.\n",
        "\n",
        "- At prediction time, the class with the highest probability is chosen.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "- Simple and works well for many problems\n",
        "\n",
        "- Supported by all solvers (e.g., liblinear)\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "- Assumes independence between classes\n",
        "\n",
        "- Can lead to ambiguous predictions if multiple classifiers give high probabilities\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- For 3 classes (A, B, C), OvR builds:\n",
        "\n",
        "  - Classifier 1: A vs (B, C)\n",
        "\n",
        "  - Classifier 2: B vs (A, C)\n",
        "\n",
        "  - Classifier 3: C vs (A, B)\n",
        "\n",
        "2.Multinomial Logistic Regression\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "- Treats the output as a single softmax probability distribution over all classes.\n",
        "\n",
        "- Uses cross-entropy loss (not binary log loss).\n",
        "\n",
        "- Optimizes all class weights simultaneously.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "- More theoretically sound for multiclass problems\n",
        "\n",
        "- Often performs better than OvR, especially when classes are not independent\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "- Not supported by all solvers (e.g., liblinear )\n",
        "\n",
        "- Supported Solvers:\n",
        "  - lbfgs, newton-cg, saga\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v892L9Ae6FTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Ans: Logistic Regression is one of the most widely used algorithms for binary and multiclass classification. It’s simple, fast, and effective in many scenarios — but it does have its trade-offs.\n",
        "\n",
        "**Advantages of Logistic Regression**\n",
        "\n",
        "| Advantage                                              | Explanation                                                                 |\n",
        "| ------------------------------------------------------ | --------------------------------------------------------------------------- |\n",
        "| **1. Simple & Interpretable**                          | Coefficients show the effect of each feature on the log-odds of the output. |\n",
        "| **2. Fast to Train**                                   | Efficient on small to medium-sized datasets.                                |\n",
        "| **3. Probabilistic Output**                            | Predicts **class probabilities**, not just hard labels.                     |\n",
        "| **4. Works Well with Linearly Separable Data**         | Performs very well if the classes are linearly separable.                   |\n",
        "| **5. Less Prone to Overfitting (with regularization)** | Regularization (L1, L2) can prevent overfitting.                            |\n",
        "| **6. Good Baseline**                                   | Often used as a strong **baseline model** before trying more complex ones.  |\n",
        "| **7. Easy to Implement & Scale**                       | Supported in all major ML libraries (scikit-learn, statsmodels, etc.).      |\n",
        "\n",
        "\n",
        "**Disadvantages of Logistic Regression**\n",
        "\n",
        "| Disadvantage                                     | Explanation                                                                                                                  |\n",
        "| ------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **1. Assumes Linearity**                         | Assumes a **linear relationship** between input features and log-odds — may underperform on nonlinear problems.              |\n",
        "| **2. Sensitive to Outliers**                     | Can be affected by extreme values unless handled with scaling or robust methods.                                             |\n",
        "| **3. Needs Feature Engineering**                 | Doesn’t automatically capture interactions or nonlinearities — you need to **manually add polynomial or interaction terms**. |\n",
        "| **4. Struggles with Multicollinearity**          | Strong correlation among input features can make coefficient estimates unstable.                                             |\n",
        "| **5. Not Ideal for Large Feature Spaces**        | In very high-dimensional settings (e.g., text classification), may be less efficient than tree-based or ensemble models.     |\n",
        "| **6. Poor Performance on Imbalanced Data**       | Without adjustments (e.g., class weights or resampling), can be **biased toward majority class**.                            |\n",
        "| **7. Can't Handle Complex Relationships Easily** | Unlike decision trees or neural networks, it doesn't model complex patterns naturally.                                       |\n"
      ],
      "metadata": {
        "id": "3whxLTJo7nl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.17.  What are some use cases of Logistic Regression?\n",
        "\n",
        "Ans: Logistic Regression is widely used in real-world applications, especially when the goal is classification and interpretable probability-based decisions. Its simplicity, efficiency, and probabilistic outputs make it useful across many domains.\n",
        "\n",
        "**Common Use Cases of Logistic Regression**\n",
        "\n",
        "1.Medical Diagnosis\n",
        "- Purpose: Predict the presence or absence of a disease (binary classification).\n",
        "\n",
        "- Example: Will a patient develop diabetes (Yes/No) based on glucose level, BMI, age, etc.?\n",
        "\n",
        "2.Credit Scoring / Loan Default Prediction\n",
        "- Purpose: Estimate the likelihood of a borrower defaulting on a loan.\n",
        "\n",
        "- Example: Predict whether a loan applicant will repay or default, based on income, credit score, debt ratio, etc.\n",
        "\n",
        "3.Email Spam Detection\n",
        "- Purpose: Classify emails as spam or not spam.\n",
        "\n",
        "- Example: Uses keywords, sender domain, frequency patterns, etc., as input features.\n",
        "\n",
        "4.Marketing & Customer Churn Prediction\n",
        "- Purpose: Predict whether a customer will:\n",
        "\n",
        "  - Respond to a marketing campaign (conversion)\n",
        "\n",
        "  - Leave the service (churn)\n",
        "\n",
        "- Example: Telecom companies predict customer churn based on usage data and complaints.\n",
        "\n",
        "5.Fraud Detection\n",
        "- Purpose: Identify potentially fraudulent transactions.\n",
        "\n",
        "- Example: A logistic regression model flags credit card transactions as fraud or legitimate based on amount, time, location, etc.\n",
        "\n",
        "6.Voting Prediction / Political Campaigns\n",
        "- Purpose: Predict whether a person will vote for a candidate or party.\n",
        "\n",
        "- Example: Use survey responses and demographics to classify voters as likely supporters.\n",
        "\n",
        "7.Ad Click Prediction\n",
        "- Purpose: Predict whether a user will click on an online advertisement.\n",
        "\n",
        "- Example: Used in real-time bidding systems in online advertising platforms.\n",
        "\n",
        "8.HR Analytics — Hiring or Attrition\n",
        "- Purpose: Predict outcomes like whether a candidate will accept a job offer, or if an employee is likely to quit.\n",
        "\n",
        "- Example: Based on job satisfaction, performance rating, years at company, etc.\n",
        "\n",
        "9.Sentiment Analysis (Binary)\n",
        "- Purpose: Classify text as positive or negative.\n",
        "\n",
        "- Example: Is this product review positive or negative?\n",
        "\n",
        "10.Industrial Applications (e.g., Fault Detection)\n",
        "- Purpose: Classify if a machine is likely to fail based on sensor readings.\n",
        "\n",
        "- Example: Predict system failure in manufacturing or energy plants."
      ],
      "metadata": {
        "id": "EK42nWfU8L76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.18.  What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Ans: The key difference between Logistic Regression and Softmax Regression lies in the number of classes they handle and how they model the output.\n",
        "\n",
        "| Feature                         | Logistic Regression                   | Softmax Regression (a.k.a. Multinomial Logistic Regression) |\n",
        "| ------------------------------- | ------------------------------------- | ----------------------------------------------------------- |\n",
        "| **Number of Classes**           | **Binary (2 classes)**                | **Multiclass (3 or more classes)**                          |\n",
        "| **Output**                      | Single probability (e.g., P(class 1)) | Probability distribution across **all classes**             |\n",
        "| **Activation Function**         | **Sigmoid**                           | **Softmax**                                                 |\n",
        "| **Loss Function**               | Binary **cross-entropy** (log loss)   | **Multiclass cross-entropy**                                |\n",
        "| **Decision Rule**               | Threshold (e.g., > 0.5 → class 1)     | Pick class with **highest probability**                     |\n",
        "| **scikit-learn Implementation** | `multi_class='ovr'` (default)         | `multi_class='multinomial'`                                 |\n"
      ],
      "metadata": {
        "id": "ZS9yy1Ng9bvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Ans: Choosing between One-vs-Rest (OvR) and Softmax (Multinomial Logistic Regression) for multiclass classification depends on data characteristics, model requirements, and performance trade-offs.\n",
        "\n",
        "**One-vs-Rest (OvR)**\n",
        "\n",
        "**When to Use:**\n",
        "\n",
        "- Small to medium-sized datasets\n",
        "\n",
        "- When using solvers that don’t support softmax (e.g., liblinear)\n",
        "\n",
        "- You want separate models per class (e.g., for interpretability)\n",
        "\n",
        "- Classes are not mutually exclusive (in some settings)\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "- Assumes classes are independent\n",
        "\n",
        "- May lead to inconsistent probability estimates\n",
        "\n",
        "- Can perform poorly if classes are highly overlapping or imbalanced\n",
        "\n",
        "**Softmax (Multinomial Logistic Regression)**\n",
        "\n",
        "**When to Use:**\n",
        "\n",
        "- Native multiclass problems where classes are mutually exclusive\n",
        "\n",
        "- You want a single unified model\n",
        "\n",
        "- You're using solvers like lbfgs, newton-cg, or saga\n",
        "\n",
        "- You're optimizing for true probability estimates\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "- Slightly slower training time than OvR\n",
        "\n",
        "- Requires more memory if the number of classes is very large\n",
        "\n",
        "**When to Prefer Each**\n",
        "\n",
        "| Situation                               | Recommended Approach                        |\n",
        "| --------------------------------------- | ------------------------------------------- |\n",
        "| Classes are **mutually exclusive**      | ✅ Softmax                                   |\n",
        "| Using a solver that supports only OvR   | ✅ OvR                                       |\n",
        "| Need **probability calibration**        | ✅ Softmax                                   |\n",
        "| Need **interpretable models per class** | ✅ OvR                                       |\n",
        "| Number of classes is **very large**     | ✅ OvR (may be more scalable)                |\n",
        "| You’re unsure?                          | ✅ Try both and compare via cross-validation |\n"
      ],
      "metadata": {
        "id": "VDK0Sp4v-0IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.20.  How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Ans: Interpreting coefficients in Logistic Regression helps you understand how each feature influences the predicted outcome — particularly the log-odds of the target class.\n",
        "\n",
        "**Interpreting the Coefficients**\n",
        "\n",
        "1.Log-Odds Interpretation\n",
        "- Each coefficient\n",
        "𝛽𝑖 represents the change in log-odds of the outcome per unit increase in 𝑥𝑖, holding all other variables constant.\n",
        "\n",
        "- Example:\n",
        "  - If 𝛽2=0.7, then for every one-unit increase in 𝑥2, the log-odds of the positive class increase by 0.7.\n",
        "\n",
        "2.Odds Ratio (Exponentiated Coefficients)\n",
        "- To make coefficients more intuitive, you can exponentiate them:\n",
        "\n",
        "  Odds Ratio=𝑒𝛽𝑖\n",
        "- This tells you how the odds (not probability) change:\n",
        "\n",
        "  - If 𝑒𝛽𝑖=2: odds are doubled for a one-unit increase in 𝑥𝑖\n",
        "  - If 𝑒𝛽𝑖=0.5: odds are halved\n",
        "\n",
        "**Signs of Coefficients**\n",
        "\n",
        "- Positive 𝛽𝑖:Increasing 𝑥𝑖 increases the likelihood of the positive class\n",
        "- Negative 𝛽𝑖: Increasing 𝑥𝑖 decreases the likelihood of the positive class\n",
        "\n",
        "- Zero 𝛽𝑖: No effect\n",
        "\n"
      ],
      "metadata": {
        "id": "LQxCB3iKAbz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "hJ-rY5BWtXv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "#Regression, and prints the model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)  # Increase max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGHyTjoHtcaY",
        "outputId": "dffd3873-e388-4c4f-a46e-d6aeb4a3b37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "#and print the model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='saga', max_iter=200, multi_class='multinomial')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"L1-Regularized Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Z3yZklt1nr",
        "outputId": "84dce705-edfd-4f1b-e0ba-8e8e531c6a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Logistic Regression Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "#LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, multi_class='multinomial')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Output accuracy and coefficients\n",
        "print(f\"L2-Regularized Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for i, class_coef in enumerate(model.coef_):\n",
        "    print(f\"Class {i}: {np.round(class_coef, 4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFOoCwStuIzw",
        "outputId": "bd3cb0e2-8870-44b8-d5fe-96a9e1b901f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-Regularized Logistic Regression Accuracy: 100.00%\n",
            "\n",
            "Model Coefficients:\n",
            "Class 0: [-0.3935  0.9625 -2.3751 -0.9987]\n",
            "Class 1: [ 0.5084 -0.2548 -0.213  -0.7757]\n",
            "Class 2: [-0.115  -0.7077  2.5881  1.7745]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,        # Elastic Net mix: 0.0 = L2 only, 1.0 = L1 only\n",
        "    max_iter=200,\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Elastic Net Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Display coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for i, class_coef in enumerate(model.coef_):\n",
        "    print(f\"Class {i}: {np.round(class_coef, 4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgkYL_FkuZQD",
        "outputId": "1cc8e118-6d76-42d4-8c50-7eb01e73f1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Logistic Regression Accuracy: 100.00%\n",
            "\n",
            "Model Coefficients:\n",
            "Class 0: [ 0.393   1.7651 -2.4207 -0.7163]\n",
            "Class 1: [ 0.0746  0.      0.     -0.5774]\n",
            "Class 2: [-1.2603 -1.5294  2.5948  2.0854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "#multi_class='ovr'\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression using One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-Rest Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 6: Show model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for i, coef in enumerate(model.coef_):\n",
        "    print(f\"Class {i} vs Rest: {np.round(coef, 4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqULFeLxupD_",
        "outputId": "f65b019b-cfff-4b3a-a2cc-2a4922147b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-Rest Logistic Regression Accuracy: 96.67%\n",
            "\n",
            "Model Coefficients:\n",
            "Class 0 vs Rest: [-0.4276  0.8877 -2.2147 -0.9161]\n",
            "Class 1 vs Rest: [-0.0339 -2.0443  0.5427 -1.0179]\n",
            "Class 2 vs Rest: [-0.389  -0.6215  2.7763  2.0907]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "#Regression. Print the best parameters and accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Set up parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Step 4: Create LogisticRegression model with compatible solver\n",
        "logreg = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=500)\n",
        "\n",
        "# Step 5: Set up GridSearchCV\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZdhyOjCu5Mb",
        "outputId": "16083bdc-cf94-4952-f6da-3d96dc009fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l1'}\n",
            "Test Set Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "#average accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Create Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='multinomial')\n",
        "\n",
        "# Step 3: Set up Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Evaluate model using cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Step 5: Print individual fold scores and average accuracy\n",
        "for i, score in enumerate(scores, 1):\n",
        "    print(f\"Fold {i} Accuracy: {score * 100:.2f}%\")\n",
        "\n",
        "print(f\"\\nAverage Accuracy: {np.mean(scores) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWEaRjOVvXDi",
        "outputId": "9f3c33d7-8b18-42b0-ae9f-31f3875104a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Accuracy: 100.00%\n",
            "Fold 2 Accuracy: 96.67%\n",
            "Fold 3 Accuracy: 93.33%\n",
            "Fold 4 Accuracy: 100.00%\n",
            "Fold 5 Accuracy: 93.33%\n",
            "\n",
            "Average Accuracy: 96.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "#accuracy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from CSV\n",
        "# Replace 'your_file.csv' with the path to your CSV file\n",
        "data = pd.read_csv('sample_data.csv')\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LUENy5QvnWu",
        "outputId": "8eef6622-49b8-4a12-b4bc-44281347856a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "#Logistic Regression. Print the best parameters and accuracy\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training/testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define search space for hyperparameters\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 2, 10),  # Try values from 0.001 to 100\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],  # These support both L1 and L2 penalties\n",
        "}\n",
        "\n",
        "# Step 4: Create the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, multi_class='auto')\n",
        "\n",
        "# Step 5: Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Step 6: Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Print results\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Nf2KZimwyC-",
        "outputId": "5bce800e-98e1-4ecf-b3f1-746b6be4f8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': np.float64(100.0)}\n",
            "Test Set Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.  Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 2: Split data into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create OvO classifier with Logistic Regression as base estimator\n",
        "ovo_clf = OneVsOneClassifier(LogisticRegression(max_iter=200, solver='lbfgs'))\n",
        "\n",
        "# Step 4: Train model\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate accuracy\n",
        "y_pred = ovo_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6xmKqdKxGPl",
        "outputId": "54c6cd2b-919b-4979-efdb-ad8dd7e0da29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "#classification\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,    # number of samples\n",
        "    n_features=20,     # number of features\n",
        "    n_informative=2,   # number of informative features\n",
        "    n_redundant=0,     # number of redundant features\n",
        "    n_classes=2,       # binary classification\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t_XTxjPUxPcZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "56584548-3d34-44ae-880c-0cc4655015d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "#Recall, and F1-Score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall:    {recall:.3f}\")\n",
        "print(f\"F1-Score:  {f1:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vFuej208n5-",
        "outputId": "06c2001f-183c-47a5-be2f-e739f0ad39b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.911\n",
            "Recall:    0.785\n",
            "F1-Score:  0.843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13.  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "#improve model performance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=10,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1 (imbalanced)\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression without class weights\n",
        "model_no_weights = LogisticRegression(max_iter=1000)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"Classification report WITHOUT class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Train logistic regression WITH class weights (balanced)\n",
        "model_with_weights = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "print(\"Classification report WITH class weights:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91fWVMIW81PO",
        "outputId": "9d75f584-02b0-47b1-f52f-2d45ff7418f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report WITHOUT class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97       224\n",
            "           1       0.87      0.50      0.63        26\n",
            "\n",
            "    accuracy                           0.94       250\n",
            "   macro avg       0.91      0.75      0.80       250\n",
            "weighted avg       0.94      0.94      0.93       250\n",
            "\n",
            "Classification report WITH class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95       224\n",
            "           1       0.53      0.88      0.67        26\n",
            "\n",
            "    accuracy                           0.91       250\n",
            "   macro avg       0.76      0.90      0.81       250\n",
            "weighted avg       0.94      0.91      0.92       250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "#evaluate performance\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Select features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "target = 'survived'\n",
        "\n",
        "X = titanic[features]\n",
        "y = titanic[target]\n",
        "\n",
        "# Handle missing values and categorical variables using pipeline\n",
        "\n",
        "# Numeric features\n",
        "numeric_features = ['age', 'sibsp', 'parch', 'fare', 'pclass']\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Categorical features\n",
        "categorical_features = ['sex', 'embarked']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create pipeline with preprocessing + Logistic Regression model\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3jlUJ1q8__d",
        "outputId": "5d0a8096-de94-4395-b740-d696a4028836"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15.   Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "#model. Evaluate its accuracy and compare results with and without scaling\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# --- Without scaling ---\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# --- With Standardization ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.3f}\")\n",
        "print(f\"Accuracy with Standardization: {acc_scaled:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyGWG0FW9SAF",
        "outputId": "5d8c5ac1-dcdc-4120-85eb-7070a0f7f1b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.848\n",
            "Accuracy with Standardization: 0.848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC score: {roc_auc:.3f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
        "plt.title(\"ROC Curve - Logistic Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "_QtuhKkr9eCH",
        "outputId": "a9a175cf-a362-481f-979d-b744affffa7f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.940\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17.  Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "#accuracy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Logistic Regression with C=0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-l2JDmN9rSN",
        "outputId": "28315a5b-7d10-40c3-9110-dd2ba2881b0f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.  Write a Python program to train Logistic Regression and identify important features based on model\n",
        "#coefficients.\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Feature names (for illustration)\n",
        "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get the coefficients (weights)\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Combine feature names with coefficients\n",
        "feature_importance = list(zip(feature_names, coefficients))\n",
        "\n",
        "# Sort features by absolute coefficient value in descending order\n",
        "feature_importance_sorted = sorted(feature_importance, key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "# Print features and their coefficients\n",
        "print(\"Feature importance based on Logistic Regression coefficients:\")\n",
        "for feature, coef in feature_importance_sorted:\n",
        "    print(f\"{feature}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x2WcXGu92O9",
        "outputId": "d22d85df-f27e-4700-aa0b-3fb78c2218c3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance based on Logistic Regression coefficients:\n",
            "feature_7: 0.7655\n",
            "feature_8: -0.7262\n",
            "feature_4: -0.7043\n",
            "feature_2: 0.6539\n",
            "feature_0: -0.2602\n",
            "feature_6: 0.1609\n",
            "feature_1: -0.0415\n",
            "feature_9: 0.0387\n",
            "feature_5: -0.0269\n",
            "feature_3: 0.0039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "#Score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvuJMojY-CN5",
        "outputId": "3914e63f-c8e1-4568-e4e6-c923278678cf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "#classification\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve - Logistic Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "tAV1JaOn-Mrn",
        "outputId": "d8330566-8d52-4cc9-ad56-fbb82ee8ef18"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "#their accuracy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store accuracy results\n",
        "accuracy_results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # Initialize Logistic Regression with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = accuracy\n",
        "\n",
        "# Print accuracy results\n",
        "print(\"Accuracy scores for different solvers:\")\n",
        "for solver, acc in accuracy_results.items():\n",
        "    print(f\"{solver}: {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhblSV5p-dMV",
        "outputId": "e334d68e-4971-4a2d-c3dc-cc748689f555"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for different solvers:\n",
            "liblinear: 0.848\n",
            "saga: 0.848\n",
            "lbfgs: 0.848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "#Correlation Coefficient (MCC)\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJfQ0tvzBEMX",
        "outputId": "1d5d665d-f644-449d-879f-4785a1b47704"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23.  Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "#accuracy to see the impact of feature scaling\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=5,\n",
        "    n_redundant=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# --- Train on raw data ---\n",
        "model_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# --- Train on standardized data ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy on raw data:         {accuracy_raw:.3f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6YGrHgHBXGP",
        "outputId": "f9edf760-5dbe-4dc7-d769-a557c12fac98"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data:         0.824\n",
            "Accuracy on standardized data: 0.824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "#cross-validation\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=5,\n",
        "    n_redundant=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "\n",
        "# Define range of C values to search over\n",
        "param_grid = {'C': [0.01, 0.1, 0.5, 1, 5, 10, 50, 100]}\n",
        "\n",
        "# Setup GridSearchCV for cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best C parameter\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Optimal C found by cross-validation: {best_C}\")\n",
        "\n",
        "# Evaluate on test set with best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with optimal C: {test_accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob0RBb-LBpZO",
        "outputId": "15f47b37-5150-4b6f-8f44-d8ec250d36a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C found by cross-validation: 0.01\n",
            "Test set accuracy with optimal C: 0.840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "#make predictions.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "print(\"Model saved to 'logistic_regression_model.joblib'\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "print(\"Model loaded from 'logistic_regression_model.joblib'\")\n",
        "\n",
        "# Use the loaded model to make predictions\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of loaded model: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KI_wXCtB27X",
        "outputId": "03f204df-f7e2-475f-8bfc-f92ab2f816ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'logistic_regression_model.joblib'\n",
            "Model loaded from 'logistic_regression_model.joblib'\n",
            "Accuracy of loaded model: 0.804\n"
          ]
        }
      ]
    }
  ]
}